---
title: Алгоритм k ближайших соседей
permalink: /algorythms/knn/index.html
---

### Область применения:
Метод k ближайших соседей (KNN) - метрический алгоритм для автоматической классификации объектов и регрессии.

___
***Регрессия*** - односторонняя зависимость, устанавливающая соответствие между случайными переменными, то есть математическое выражение, отражающее связь между зависимой переменной ***у*** и независимыми переменными ***х*** при условии, что это выражение будет иметь статистическую значимость.
___

Алгоритма k ближайших соседей классифицирует объект и присваивать его тому классу, который является наиболее распространённым среди k соседей данного элемента, при этом классы соседей уже известны.

В случае использования метода для регрессии, объекту присваивается среднее значение по k ближайшим к нему объектам, значения которых уже известны.

Алгоритм может быть применим к выборкам с большим количеством атрибутов (многомерным). Для этого перед применением нужно определить функцию расстояния.

Алгоритм KNN может применяться практически во всех задачах классификации, особенно в тех случаях, когда оценить параметры вероятностного распределения данных сложно или невозможно. 

Наиболее типичными приложениями алгоритма KNN являются:
* Классификация клиентов (например, по уровню лояльности);
* Медицина - классификация пациентов по медицинским показателям;
* Маркетинг - классификация товаров по уровню популярности и т.д.

* Метод был впервые разработан Эвелин Фикс и Джозефом Ходжесом-младшим в 1951 году, и позднее развит Томасом Ковером.

### Идея:

Метод k-ближайших соседей относится к классу непараметрических, т.е. не требует предположений о том, из какого статистического распределения была сформирована обучающее множество. Следовательно, классификационные модели, построенные с помощью метода также будут непараметрическими. Это означает, что структура модели не задаётся жёстко изначально, а определяется данными. Поскольку признаки, на основе которых производится классификация могут иметь различную физическую природу и, соответственно, диапазоны значений, для улучшения результатов классификации будет полезно выполнить нормализацию обучающих данных.


Перейдём к описанию алгоритма: пусть имеется набор данных, состоящий из n наблюдений Xi(i=1,...,n), для каждого из которых задан класс Cj(j=1,...,m). Тогда на его основе может быть сформировано обучающее множество, все примеры которого представляют собой пары Xi,Cj.

***Алгоритм KNN можно разделить на две простые фазы:*** 
- обучение и классификация;

При *обучении* алгоритм просто запоминает векторы признаков наблюдений и их метки классов (т.е. примеры). Также задаётся параметр алгоритма k, который задаёт число «соседей», которые будут использоваться при классификации.
На *фазе классификации* предъявляется новый объект, для которого метка класса не задана. Для него определяются k ближайших (в смысле некоторой метрики) предварительно классифицированных наблюдений. Затем выбирается класс, которому принадлежит большинство из k ближайших примеров-соседей, и к этому же классу относится классифицируемый объект.
*На рисунке показано более наглядно:*
![Картинка к алгоритму](https://loginom.ru/sites/default/files/blogpost-files/knn-working.svg)
*Поясним рисунок:* кружком представлен объект, который требуется классифицировать в один из двух классов «треугольники» и «квадраты». Если выбрать k=3, то из трёх ближайших объектов два окажутся «треугольниками» и один «квадратом». Следовательно, новому объекту будет присвоен класс «треугольник». Если задать k=5, то из пяти «соседей» два будут «треугольники» и три «квадраты», в результате классифицируемый объект будет распознан как «квадрат».
В простейшем случае класс нового объекта может быть определён простым выбором наиболее часто встречающегося класса среди k примеров. Однако на практике это не всегда удачное решение, например, в случае когда частота появления для двух или более классов оказывается одинаковой. Кроме этого разумно предположить, что не все обучающие имеют одинаковую значимость для определения класса. В этом случае используют некоторую функцию, с помощью которой определяется класс, называемую функцией сочетания (combination function).
В обычном случае используют так называемое *простое невзвешенное голосование* (simple unweighted voting). При этом предполагается, что все k примеров имеют одинаковое право «голоса» независимо от расстояния до классифицируемого объекта.
Однако, логично предположить, что чем дальше пример расположен от классифицируемого объекта в пространстве признаков, тем ниже его значимость для определения класса. Поэтому для улучшения результатов классификации вводят взвешивание примеров в зависимости от их удалённости. В этом случае используют взвешенное голосование (weighted voting).
*В основе идеи взвешенного голосования лежит* введение «штрафа» для класса, в зависимости от того, насколько относящиеся к нему примеры удалены от классифицируемого объекта. Такой «штраф» представляется как сумма величин, обратных квадрату расстояний от примера j-го класса до классифицируемого объекта (часто данное значение называют показателем близости). 

Таким образом, «побеждает» тот класс, для которого величина Qj окажется наибольшей. При этом также снижается вероятность того, что классы получат одинаковое число голосов.

Стоит выделить, что выбор параметра k является важным для получения корректных результатов классификации. Если значение параметра мало, то возникает эффект переобучения, когда решение по классификации принимается на основе малого числа примеров и имеет низкую значимость. Это похоже на переобучение в деревьях решений, когда в них много правил, относящихся к небольшому числу примеров. Если установить k=1, то алгоритм будет просто присваивать любому новому наблюдению метку класса ближайшего объекта.
Кроме этого, следует учитывать, что использование небольших значений k увеличивает влияние шумов на результаты классификации, когда небольшие изменения в данных приводят к большим изменениям в результатах классификации. Но при этом границы классов оказываются более выраженными (класс при голосовании побеждает с большим счётом). Напротив, если значение параметра слишком велико, то в процессе классификации принимает участие много объектов, относящихся к разным классам. Такая классификация оказывается слишком грубой и плохо отражает локальные особенности набора данных. 

В результате выбор параметра k является компромиссом между точностью и обобщающей способностью модели. При больших значениях параметра k уменьшается зашумленность результатов классификации, но снижается выраженность границ классов.

В задачах бинарной классификации целесообразно выбрать k как нечетное число, так как это позволяет избежать равенства «голосов» при определении класса для нового наблюдения.

***Стоит выделить некоторые особенности работы алгоритма:***
- Если значения признаков непрерывные, то в качестве меры расстояния между объектами обычно используется расстояние Евклида, а если категориальные, то может использоваться расстояние Хэмминга.
- Алгоритм является чувствительным к дисбалансу классов в обучающих данных: алгоритм «склонен» к смещению решения в сторону доминирующего класса, поскольку относящиеся к нему объекты просто чаще попадают в число ближайших соседей. 

Одним из способов решения данной проблемы является применение различных способов взвешивания при «голосовании».
- Следует отметить, что отношение соседства не является коммутативным, т.е. если для записи ***B*** ближайшим соседом является запись ***A***, то это не означает, что ***B – ближайший сосед A***. 
Такая ситуация представлена на рисунке, её обычно называют ситуацией с обратным соседством:
![Ситуация с обратным соседством](https://loginom.ru/sites/default/files/blogpost-files/knn-distance.svg)

Рассмотрим этот случай подробнее: при k=1 ближайшей для точки B будет точка A, а для А — точка C. Даже при увеличении коэффициента до k=7, точка B по-прежнему не будет входить в число соседей A.

***Отметим достоинства и недостатки алгоритма:***
К *достоинствам* алгоритма можно отнести:
* Устойчивость к выбросам и аномальным значениям, поскольку вероятность попадания содержащих их записей в число k-ближайших соседей мала. Если же это произошло, то влияние на голосование (особенно взвешенное) также, скорее всего, будет незначительным, и, следовательно, малым будет и влияние на результаты классификации;
* Реализация алгоритма относительно проста;
* Результаты работы алгоритма легко поддаются интерпретации.

К *недостаткам* алгоритм KNN можно отнести:
* Данный метод не создает каких-либо моделей, обобщающих предыдущий опыт, а интерес могут представлять и сами правила классификации;
* При классификации объекта используются все доступные данные, поэтому метод является достаточно затратным в вычислительном плане, особенно в случае больших объёмов данных;
* Высокая трудоёмкость из-за необходимости вычисления расстояний до всех примеров;
* Повышенные требования к репрезентативности исходных данных.

### Реализации на разных языках:

***Python***
```sh
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
path = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
dataset = pd.read_csv(path, names = headernames)
dataset.head()
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 8)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
result = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(result)
result1 = classification_report(y_test, y_pred)
print("Classification Report:",)
print (result1)
result2 = accuracy_score(y_test,y_pred)
print("Accuracy:",result2)

# Выход
# Confusion Matrix:
# [[21 0 0]
# [ 0 16 0]
# [ 0 7 16]]
# Classification Report:
# precision   recall   f1-score   support
# Iris-setosa        1.00     1.00       1.00        21
# Iris-versicolor        0.70     1.00       0.82        16
# Iris-virginica        1.00     0.70       0.82        23
# micro avg        0.88     0.88       0.88        60
# macro avg        0.90     0.90       0.88        60
# weighted avg        0.92     0.88       0.88        60
#
# Accuracy: 0.8833333333333333
```

***Реализация классификации в качестве регрессора на Python:***
```sh
import numpy as np
import pandas as pd
path = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
data = pd.read_csv(url, names = headernames)
array = data.values
X = array[:,:2]
Y = array[:,2]
data.shape
output: (150, 5)
from sklearn.neighbors import KNeighborsRegressor
knnr = KNeighborsRegressor(n_neighbors = 10)
knnr.fit(X, y)
print ("The MSE is:",format(np.power(y-knnr.predict(X),2).mean()))
# Выход
# The MSE is: 0.12226666666666669
```

***C#***
```sh
%% Example 1
% Example, which is simply classifed by WeightedKNN. It seems like XOR
% problem.

%% generate training sample
% generate 2 groups of normal classes. Each group consistes of 2 simple
% normal classes
XL1 = [GetNormClass(50,[0,0],[1,1]); GetNormClass(50,[6,6],[1,1])];
XL2 = [GetNormClass(50,[6,0],[1,1]); GetNormClass(50,[0,6],[1,1])];
XL = [XL1; XL2];
YL = [repmat(1,100,1);repmat(2,100,1)];

%% generate control data with the same distribution
X1 = [GetNormClass(50,[0,0],[1,1]); GetNormClass(50,[6,6],[1,1])];
X2 = [GetNormClass(50,[6,0],[1,1]); GetNormClass(50,[0,6],[1,1])];
X = [X1; X2];
Y = [repmat(1,100,1);repmat(2,100,1)];

%% get classification
%% choosing parametrs
PP = ParAdjust(XL, YL);
PP.XL = XL;
PP.YL = YL;
%% classification
Y = WeightKNN(X, PP);

%% results visuaisation
%% plotting real classes of objects
plot(X1(:,1),X1(:,2),'*r');
hold on
plot(X2(:,1),X2(:,2),'*b');

%% plotting classification results
plot(X(Y == 1,1),X(Y == 1,2),'or');
plot(X(Y == 2,1),X(Y == 2,2),'ob');
hold off
```

***Java***
```sh
First prepare your data by creating a txt file "ads.txt":
@relation ads

@attribute pictures numeric
@attribute paragraphs numeric
@attribute profit {Y, N}

@data
10,2,Y
12,3,Y
9,2,Y
0,10,N
1,9,N
3,11,N

Java Code:
import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;

import weka.classifiers.Classifier;
import weka.classifiers.lazy.IBk;
import weka.core.Instance;
import weka.core.Instances;

public class KNN {
public static BufferedReader readDataFile(String filename) {
BufferedReader inputReader = null;

		try {
			inputReader = new BufferedReader(new FileReader(filename));
		} catch (FileNotFoundException ex) {
			System.err.println("File not found: " + filename);
		}
 
		return inputReader;
	}
 
	public static void main(String[] args) throws Exception {
		BufferedReader datafile = readDataFile("ads.txt");
 
		Instances data = new Instances(datafile);
		data.setClassIndex(data.numAttributes() - 1);
 
		//do not use first and second
		Instance first = data.instance(0);
		Instance second = data.instance(1);
		data.delete(0);
		data.delete(1);
 
		Classifier ibk = new IBk();		
		ibk.buildClassifier(data);
 
		double class1 = ibk.classifyInstance(first);
		double class2 = ibk.classifyInstance(second);
 
		System.out.println("first: " + class1 + "\nsecond: " + class2);
	}
}
```

### Список источников:
Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. — Springer, 2001.
